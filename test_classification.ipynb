{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "NE1aLuRL9AET",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Install the library\n",
    "! pip3 install --upgrade --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: google-cloud-aiplatform\n",
      "Version: 1.46.0\n",
      "Summary: Vertex AI API client library\n",
      "Home-page: https://github.com/googleapis/python-aiplatform\n",
      "Author: Google LLC\n",
      "Author-email: googleapis-packages@google.com\n",
      "License: Apache 2.0\n",
      "Location: /opt/conda/lib/python3.10/site-packages\n",
      "Requires: docstring-parser, google-api-core, google-auth, google-cloud-bigquery, google-cloud-resource-manager, google-cloud-storage, packaging, proto-plus, protobuf, pydantic, shapely\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "vdsw5Lui9B5p",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"vddl-419615\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m You do not appear to have access to project [vddl-419615] or it does not exist.\n",
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "# Set the project id (--quiet - resolves the prompt automatically)\n",
    "! gcloud config set project {PROJECT_ID} --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vddl-419615\n"
     ]
    }
   ],
   "source": [
    "#Check the vallue of the project\n",
    "! gcloud config get-value project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "ZAhrjReK9JZ_"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "jmhNY9A_9L6Z",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an0zf8ux\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
    "\n",
    "\n",
    "UUID = generate_uuid()\n",
    "print(UUID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "pWqqqwf79Qmi",
    "outputId": "52a3c94a-9b6e-4573-8934-b3deac0156b8"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://vddl-test-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cPE4s8Sy9TlI",
    "outputId": "db188f11-f983-488c-8ee0-523587a94bc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://vddl-test-vddl-419615-unique/...\n"
     ]
    }
   ],
   "source": [
    "#Create a bucket in the specified location and project with the given name using Google Cloud Storage\n",
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "fHFZNp0z9WYv"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import google.cloud.aiplatform as aip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "AxInTktQ9YRC",
    "outputId": "9fd22d82-9693-4dcd-fd60-0017361b58d9"
   },
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(os.getenv(\"IS_TESTING_TRAIN_GPU\"))\n",
    "print(os.getenv(\"IS_TESTING_DEPLOY_GPU\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "3g3Z0YXt9ZgU"
   },
   "outputs": [],
   "source": [
    "if os.getenv(\"IS_TESTING_TRAIN_GPU\"):\n",
    "    TRAIN_GPU, TRAIN_NGPU = (\n",
    "        aip.gapic.AcceleratorType.ACCELERATOR_TYPE_UNSPECIFIED,\n",
    "        int(os.getenv(\"IS_TESTING_TRAIN_GPU\")),\n",
    "    )\n",
    "else:\n",
    "    TRAIN_GPU, TRAIN_NGPU = (aip.gapic.AcceleratorType.ACCELERATOR_TYPE_UNSPECIFIED, 0)\n",
    "\n",
    "if os.getenv(\"IS_TESTING_DEPLOY_GPU\"):\n",
    "    DEPLOY_GPU, DEPLOY_NGPU = (\n",
    "        aip.gapic.AcceleratorType.ACCELERATOR_TYPE_UNSPECIFIED,\n",
    "        int(os.getenv(\"IS_TESTING_DEPLOY_GPU\")),\n",
    "    )\n",
    "else:\n",
    "    DEPLOY_GPU, DEPLOY_NGPU = (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(os.getenv(\"IS_TESTING_TF\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "ZpZ0kYqH9bPh",
    "outputId": "87159afb-9ba7-481c-9329-88485f37e147"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-5:latest AcceleratorType.ACCELERATOR_TYPE_UNSPECIFIED 0\n",
      "Deployment: us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-5:latest None None\n"
     ]
    }
   ],
   "source": [
    "if os.getenv(\"IS_TESTING_TF\"):\n",
    "    TF = os.getenv(\"IS_TESTING_TF\")\n",
    "else:\n",
    "    TF = \"2.5\".replace(\".\", \"-\")\n",
    "\n",
    "if TF[0] == \"2\":\n",
    "    if TRAIN_GPU:\n",
    "        TRAIN_VERSION = \"tf-gpu.{}\".format(TF)\n",
    "    else:\n",
    "        TRAIN_VERSION = \"tf-cpu.{}\".format(TF)\n",
    "    if DEPLOY_GPU:\n",
    "        DEPLOY_VERSION = \"tf2-gpu.{}\".format(TF)\n",
    "    else:\n",
    "        DEPLOY_VERSION = \"tf2-cpu.{}\".format(TF)\n",
    "else:\n",
    "    if TRAIN_GPU:\n",
    "        TRAIN_VERSION = \"tf-gpu.{}\".format(TF)\n",
    "    else:\n",
    "        TRAIN_VERSION = \"tf-cpu.{}\".format(TF)\n",
    "    if DEPLOY_GPU:\n",
    "        DEPLOY_VERSION = \"tf-gpu.{}\".format(TF)\n",
    "    else:\n",
    "        DEPLOY_VERSION = \"tf-cpu.{}\".format(TF)\n",
    "\n",
    "TRAIN_IMAGE = \"{}-docker.pkg.dev/vertex-ai/training/{}:latest\".format(\n",
    "    REGION.split(\"-\")[0], TRAIN_VERSION\n",
    ")\n",
    "DEPLOY_IMAGE = \"{}-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(\n",
    "    REGION.split(\"-\")[0], DEPLOY_VERSION\n",
    ")\n",
    "\n",
    "print(\"Training:\", TRAIN_IMAGE, TRAIN_GPU, TRAIN_NGPU)\n",
    "print(\"Deployment:\", DEPLOY_IMAGE, DEPLOY_GPU, DEPLOY_NGPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "onpwIc6b9ecX",
    "outputId": "ad74fea4-b241-4d39-dbc2-b7b5614941d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train machine type n1-standard-4\n"
     ]
    }
   ],
   "source": [
    "if os.getenv(\"IS_TESTING_TRAIN_MACHINE\"):\n",
    "    MACHINE_TYPE = os.getenv(\"IS_TESTING_TRAIN_MACHINE\")\n",
    "else:\n",
    "    MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "TRAIN_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Train machine type\", TRAIN_COMPUTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "lu028urx9gOQ"
   },
   "outputs": [],
   "source": [
    "# Make folder for Python training script\n",
    "! rm -rf custom\n",
    "! mkdir custom\n",
    "\n",
    "# Add package information\n",
    "! touch custom/README.md\n",
    "\n",
    "setup_cfg = \"[egg_info]\\n\\ntag_build =\\n\\ntag_date = 0\"\n",
    "! echo \"$setup_cfg\" > custom/setup.cfg\n",
    "\n",
    "setup_py = \"import setuptools\\n\\nsetuptools.setup(\\n\\n    install_requires=[\\n\\n        'tensorflow==2.5.0',\\n\\n        'tensorflow_datasets==1.3.0',\\n\\n    ],\\n\\n    packages=setuptools.find_packages())\"\n",
    "! echo \"$setup_py\" > custom/setup.py\n",
    "\n",
    "pkg_info = \"Metadata-Version: 1.0\\n\\nName: Classification cloud\\n\\nVersion: 0.0.0\\n\\nSummary: Demonstration training script\\n\\nHome-page: www.google.com\\n\\nAuthor: Google\\n\\nAuthor-email: aferlitsch@google.com\\n\\nLicense: Public\\n\\nDescription: Demo\\n\\nPlatform: Vertex\"\n",
    "! echo \"$pkg_info\" > custom/PKG-INFO\n",
    "\n",
    "# Make the training subfolder\n",
    "! mkdir custom/trainer\n",
    "! touch custom/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XWzV4zi99jm8",
    "outputId": "b7c3fe7f-50c8-44f2-ae7d-96e436f9ecbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom/trainer/task.py\n",
    "# Single, Mirrored and MultiWorker Distributed Training\n",
    "\n",
    "import os\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.python.client import device_lib\n",
    "import numpy as np\n",
    "import argparse\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model-dir', dest='model_dir',\n",
    "                    default=os.getenv('AIP_MODEL_DIR'), type=str, help='Model dir.')\n",
    "parser.add_argument('--lr', dest='lr',\n",
    "                    default=0.001, type=float,\n",
    "                    help='Learning rate.')\n",
    "parser.add_argument('--epochs', dest='epochs',\n",
    "                    default=10, type=int,\n",
    "                    help='Number of epochs.')\n",
    "parser.add_argument('--steps', dest='steps',\n",
    "                    default=100, type=int,\n",
    "                    help='Number of steps per epoch.')\n",
    "parser.add_argument('--batch_size', dest='batch_size',\n",
    "                    default=16, type=int,\n",
    "                    help='Size of a batch.')\n",
    "parser.add_argument('--distribute', dest='distribute', type=str, default='single',\n",
    "                    help='distributed training strategy')\n",
    "parser.add_argument('--param-file', dest='param_file',\n",
    "                    default='/tmp/param.txt', type=str,\n",
    "                    help='Output file for parameters')\n",
    "args = parser.parse_args()\n",
    "\n",
    "logging.info('DEVICES'  + str(device_lib.list_local_devices()))\n",
    "\n",
    "# Single Machine, single compute device\n",
    "if args.distribute == 'single':\n",
    "    if tf.test.is_gpu_available():\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "    else:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "    logging.info(\"Single device training\")\n",
    "# Single Machine, multiple compute device\n",
    "elif args.distribute == 'mirrored':\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    logging.info(\"Mirrored Strategy distributed training\")\n",
    "# Multi Machine, multiple compute device\n",
    "elif args.distribute == 'multiworker':\n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "    logging.info(\"Multi-worker Strategy distributed training\")\n",
    "    logging.info('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n",
    "    # Single Machine, multiple TPU devices\n",
    "elif args.distribute == 'tpu':\n",
    "    cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=\"local\")\n",
    "    tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
    "    tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
    "    strategy = tf.distribute.TPUStrategy(cluster_resolver)\n",
    "    print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
    "\n",
    "logging.info('num_replicas_in_sync = {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "\n",
    "def get_compiled_model():\n",
    "    # Make a simple 2-layer densely-connected neural network.\n",
    "    inputs = keras.Input(shape=(784,))\n",
    "    x = keras.layers.Dense(256, activation=\"relu\")(inputs)\n",
    "    x = keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "    outputs = keras.layers.Dense(10)(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_dataset():\n",
    "    batch_size = 32\n",
    "    num_val_samples = 10000\n",
    "\n",
    "    # Return the MNIST dataset in the form of a `tf.data.Dataset`.\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "    # Preprocess the data (these are Numpy arrays)\n",
    "    x_train = x_train.reshape(-1, 784).astype(\"float32\") / 255\n",
    "    x_test = x_test.reshape(-1, 784).astype(\"float32\") / 255\n",
    "    y_train = y_train.astype(\"float32\")\n",
    "    y_test = y_test.astype(\"float32\")\n",
    "\n",
    "    # Reserve num_val_samples samples for validation\n",
    "    x_val = x_train[-num_val_samples:]\n",
    "    y_val = y_train[-num_val_samples:]\n",
    "    x_train = x_train[:-num_val_samples]\n",
    "    y_train = y_train[:-num_val_samples]\n",
    "    return (\n",
    "        tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size),\n",
    "        tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size),\n",
    "        tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size),\n",
    "    )\n",
    "\n",
    "# Create a MirroredStrategy.\n",
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# Open a strategy scope.\n",
    "with strategy.scope():\n",
    "    # Everything that creates variables should be under the strategy scope.\n",
    "    # In general this is only model construction & `compile()`.\n",
    "    model = get_compiled_model()\n",
    "\n",
    "    # Train the model on all available devices.\n",
    "    train_dataset, val_dataset, test_dataset = get_dataset()\n",
    "    model.fit(train_dataset, epochs=args.epochs, validation_data=val_dataset)\n",
    "\n",
    "    # Test the model on all available devices.\n",
    "    model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "28YQ6A839k4j",
    "outputId": "224ca4a5-0d92-458c-933c-f6ca02906e3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom/\n",
      "custom/README.md\n",
      "custom/setup.cfg\n",
      "custom/PKG-INFO\n",
      "custom/setup.py\n",
      "custom/trainer/\n",
      "custom/trainer/__init__.py\n",
      "custom/trainer/.ipynb_checkpoints/\n",
      "custom/trainer/.ipynb_checkpoints/task-checkpoint.py\n",
      "custom/trainer/task.py\n",
      "Copying file://custom.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  2.3 KiB/  2.3 KiB]                                                \n",
      "Operation completed over 1 objects/2.3 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "! rm -f custom.tar custom.tar.gz\n",
    "! tar cvf custom.tar custom\n",
    "! gzip custom.tar\n",
    "! gsutil cp custom.tar.gz $BUCKET_URI/model/classification.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "qA_Gz0sN9mqK",
    "outputId": "4c445cef-dda8-45d9-a34b-0b165815ed4c"
   },
   "outputs": [],
   "source": [
    "DISPLAY_NAME = \"classification_\" + UUID\n",
    "\n",
    "job = aip.CustomPythonPackageTrainingJob(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    python_package_gcs_uri=f\"{BUCKET_URI}/model/classification.tar.gz\",\n",
    "    python_module_name=\"trainer.task\",\n",
    "    container_uri=TRAIN_IMAGE,\n",
    "    model_serving_container_image_uri=DEPLOY_IMAGE,\n",
    "    project=PROJECT_ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "zkrA0suT9shi",
    "outputId": "9d1e020e-1c2b-4c5a-a1e1-d79ef5f94312"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://vddl-test-vddl-419615-unique\n"
     ]
    }
   ],
   "source": [
    "MODEL_DIR = BUCKET_URI\n",
    "\n",
    "print(MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Output directory:\n",
      "gs://vddl-test-vddl-419615-unique \n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/7635285854566481920?project=170952101248\n",
      "CustomPythonPackageTrainingJob projects/170952101248/locations/us-central1/trainingPipelines/7635285854566481920 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/5069210433290764288?project=170952101248\n",
      "CustomPythonPackageTrainingJob projects/170952101248/locations/us-central1/trainingPipelines/7635285854566481920 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/170952101248/locations/us-central1/trainingPipelines/7635285854566481920 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/170952101248/locations/us-central1/trainingPipelines/7635285854566481920 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/170952101248/locations/us-central1/trainingPipelines/7635285854566481920 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/170952101248/locations/us-central1/trainingPipelines/7635285854566481920 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/170952101248/locations/us-central1/trainingPipelines/7635285854566481920 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob projects/170952101248/locations/us-central1/trainingPipelines/7635285854566481920 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomPythonPackageTrainingJob run completed. Resource name: projects/170952101248/locations/us-central1/trainingPipelines/7635285854566481920\n",
      "Model available at projects/170952101248/locations/us-central1/models/1829131051293736960\n"
     ]
    }
   ],
   "source": [
    "CMDARGS = [\"--epochs=5\", \"--batch_size=16\", \"--distribute=multiworker\"]\n",
    "\n",
    "try:\n",
    "    model = job.run(\n",
    "        model_display_name=\"classification_\" + UUID,\n",
    "        args=CMDARGS,\n",
    "        replica_count=4,\n",
    "        machine_type=TRAIN_COMPUTE,\n",
    "        accelerator_type=TRAIN_GPU.name,\n",
    "        accelerator_count=TRAIN_NGPU,\n",
    "        base_output_dir=MODEL_DIR,\n",
    "        sync=True,\n",
    "    )\n",
    "except Exception as e:\n",
    "    # may fail duing model.save() -- seems to be some issue when merging checkpoints from the workers\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "PuKnjOel9t3p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.aiplatform.models.Model object at 0x7f164354a080> \n",
      "resource name: projects/170952101248/locations/us-central1/models/1829131051293736960\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.delete()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m119"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
